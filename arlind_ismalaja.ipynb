{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from collections import defaultdict\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Decision Tree:\n",
      "Accuracy: 0.963\n",
      "Precision: 0.984\n",
      "Recall: 0.957\n",
      "\n",
      "Results for KNN:\n",
      "Accuracy: 0.935\n",
      "Precision: 0.971\n",
      "Recall: 0.923\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.967\n",
      "Precision: 0.978\n",
      "Recall: 0.968\n",
      "\n",
      "Results for Logistic Regression:\n",
      "Accuracy: 0.919\n",
      "Precision: 0.945\n",
      "Recall: 0.923\n",
      "\n",
      "Results for Naive Bayes:\n",
      "Accuracy: 0.887\n",
      "Precision: 0.907\n",
      "Recall: 0.909\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 1\n",
    "data = pd.read_csv('diabetes_risk_prediction.csv')\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "binary_columns = ['Polyuria', 'Polydipsia', 'sudden weight loss', 'weakness', \n",
    "                 'Polyphagia', 'Genital thrush', 'visual blurring', 'Itching',\n",
    "                 'Irritability', 'delayed healing', 'partial paresis', \n",
    "                 'muscle stiffness', 'Alopecia', 'Obesity']\n",
    "\n",
    "# Convert binary columns to integers\n",
    "for column in binary_columns:\n",
    "    data[column] = (data[column] == 'Yes').astype(int)\n",
    "\n",
    "# Define the label to be predicted\n",
    "le = LabelEncoder()\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    "\n",
    "# Convert the label to be predicted to integers\n",
    "data['class'] = (data['class'] == 'Positive').astype(int)\n",
    "\n",
    "X = data.drop('class', axis=1).values\n",
    "y = data['class'].values\n",
    "\n",
    "def evaluate_model(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    metrics = {'accuracy': [], 'precision': [], 'recall': []}\n",
    "    \n",
    "    # Iterate over the folds \n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        metrics['precision'].append(precision_score(y_test, y_pred))\n",
    "        metrics['recall'].append(recall_score(y_test, y_pred))\n",
    "        \n",
    "    return {k: np.mean(v) for k, v in metrics.items()}\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    results[name] = evaluate_model(model, X, y)\n",
    "    for metric, value in results[name].items():\n",
    "        print(f\"{metric.capitalize()}: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PageRank values:\n",
      "Node 1: 0.2180\n",
      "Node 2: 0.0832\n",
      "Node 3: 0.0832\n",
      "Node 4: 0.1809\n",
      "Node 5: 0.0568\n",
      "Node 6: 0.2313\n",
      "Node 7: 0.1466\n",
      "\n",
      "Nodes sorted by PageRank (highest to lowest):\n",
      "Node 6 > Node 1 > Node 4 > Node 7 > Node 2 > Node 3 > Node 5\n",
      "\n",
      "Detailed ranking:\n",
      "1. Node 6 (PageRank: 0.2313)\n",
      "2. Node 1 (PageRank: 0.2180)\n",
      "3. Node 4 (PageRank: 0.1809)\n",
      "4. Node 7 (PageRank: 0.1466)\n",
      "5. Node 2 (PageRank: 0.0832)\n",
      "6. Node 3 (PageRank: 0.0832)\n",
      "7. Node 5 (PageRank: 0.0568)\n"
     ]
    }
   ],
   "source": [
    "# PROBELM 2\n",
    "def create_transition_matrix(adjacency_list, n_nodes):\n",
    "    \"\"\"\n",
    "    Create a transition matrix from an adjacency list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the transition matrix\n",
    "    M = np.zeros((n_nodes, n_nodes))\n",
    "    \n",
    "    # Iterate over each node in the adjacency list\n",
    "    for i in range(n_nodes):\n",
    "        out_degree = len(adjacency_list[i])\n",
    "        # If the node has outgoing edges\n",
    "        # assign the corresponding probabilities\n",
    "        if out_degree > 0:\n",
    "            for j in adjacency_list[i]:\n",
    "                M[j-1][i] = 1.0 / out_degree\n",
    "    \n",
    "    return M\n",
    "\n",
    "def pagerank(M, num_iterations=100, d=0.85):\n",
    "    \"\"\"\n",
    "    Compute the PageRank values using the power iteration method.\n",
    "    \"\"\"\n",
    "\n",
    "    n = M.shape[0]\n",
    "    \n",
    "    pr = np.ones(n) / n\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    for _ in range(num_iterations):\n",
    "        # Compute the next PageRank values\n",
    "        pr_next = (1 - d) / n + d * M @ pr\n",
    "        \n",
    "        if np.allclose(pr, pr_next):\n",
    "            break\n",
    "            \n",
    "        pr = pr_next\n",
    "    \n",
    "    return pr\n",
    "\n",
    "# Define the adjacency list for the graph\n",
    "adjacency_list = {\n",
    "    0: [2, 3, 4],\n",
    "    1: [4, 5],\n",
    "    2: [6],\n",
    "    3: [6, 7],\n",
    "    4: [7],\n",
    "    5: [1],\n",
    "    6: [4, 6]\n",
    "}\n",
    "\n",
    "n_nodes = 7\n",
    "\n",
    "M = create_transition_matrix(adjacency_list, n_nodes)\n",
    "pagerank_values = pagerank(M)\n",
    "node_rankings = [(i+1, pr) for i, pr in enumerate(pagerank_values)]\n",
    "sorted_rankings = sorted(node_rankings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nPageRank values:\")\n",
    "for node, value in node_rankings:\n",
    "    print(f\"Node {node}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nNodes sorted by PageRank (highest to lowest):\")\n",
    "print(\" > \".join([f\"Node {node}\" for node, _ in sorted_rankings]))\n",
    "\n",
    "print(\"\\nDetailed ranking:\")\n",
    "for rank, (node, value) in enumerate(sorted_rankings, 1):\n",
    "    print(f\"{rank}. Node {node} (PageRank: {value:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part a: Frequent itemsets with minSupport = 0.15\n",
      "\n",
      "Frequent 1-itemsets:\n",
      "Items: [1], Support: 0.200\n",
      "Items: [3], Support: 0.455\n",
      "Items: [6], Support: 0.607\n",
      "Items: [11], Support: 0.368\n",
      "\n",
      "Frequent 2-itemsets:\n",
      "Items: [6, 11], Support: 0.327\n",
      "Items: [3, 6], Support: 0.268\n",
      "Items: [3, 11], Support: 0.163\n",
      "\n",
      "Part b: Maximum support value for size-4 itemset: 0.050\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 3\n",
    "def load_transactions(data):\n",
    "    \"\"\"\n",
    "    Load transactions from a string\n",
    "    \"\"\"\n",
    "    transactions = []\n",
    "    for line in data.strip().split('\\n'):\n",
    "        transaction = set(map(int, line.strip().split()))\n",
    "        transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "def apriori(transactions, min_support):\n",
    "    \"\"\"\n",
    "    Find frequent itemsets using the Apriori algorithm\n",
    "    \"\"\"\n",
    "    # Count items\n",
    "    item_counts = defaultdict(int)\n",
    "    total_transactions = len(transactions)\n",
    "    \n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_counts[item] += 1\n",
    "    \n",
    "    # Find frequent items\n",
    "    frequent_items = {\n",
    "        frozenset([item]): count/total_transactions \n",
    "        for item, count in item_counts.items() \n",
    "        if count/total_transactions >= min_support\n",
    "    }\n",
    "    \n",
    "    # Find frequent itemsets\n",
    "    all_frequent_itemsets = {1: frequent_items} # Store frequent itemsets for each size\n",
    "    current_itemsets = frequent_items\n",
    "    \n",
    "    k = 2 # Current itemset size\n",
    "    # Continue until no frequent itemsets are found or until the maximum transaction length is reached\n",
    "    while current_itemsets and k <= max(len(t) for t in transactions):\n",
    "        candidates = set()\n",
    "        for item1, item2 in combinations(current_itemsets.keys(), 2):\n",
    "            union = item1.union(item2)\n",
    "            if len(union) == k:\n",
    "                candidates.add(union)\n",
    "        \n",
    "        # Count candidate itemsets\n",
    "        itemset_counts = defaultdict(int)\n",
    "        for transaction in transactions:\n",
    "            for candidate in candidates:\n",
    "                if candidate.issubset(transaction):\n",
    "                    itemset_counts[candidate] += 1\n",
    "        \n",
    "        # Find frequent itemsets\n",
    "        current_itemsets = {\n",
    "            itemset: count/total_transactions\n",
    "            for itemset, count in itemset_counts.items()\n",
    "            if count/total_transactions >= min_support\n",
    "        }\n",
    "        \n",
    "        if current_itemsets:\n",
    "            all_frequent_itemsets[k] = current_itemsets # Store frequent itemsets for this size\n",
    "            \n",
    "        k += 1\n",
    "    \n",
    "    return all_frequent_itemsets\n",
    "\n",
    "def find_max_support_size4(transactions, precision=0.001):\n",
    "    \"\"\"\n",
    "    Find the maximum support value for a size-4 itemset\n",
    "    \"\"\"\n",
    "    left = 0.0\n",
    "    right = 1.0\n",
    "    max_support = 0.0\n",
    "    \n",
    "    # Binary search for the maximum support value\n",
    "    while right - left >= precision:\n",
    "        mid = (left + right) / 2\n",
    "        frequent_sets = apriori(transactions, mid)\n",
    "        \n",
    "        has_size4 = any(k >= 4 for k in frequent_sets.keys())\n",
    "        \n",
    "        if has_size4:\n",
    "            max_support = mid\n",
    "            left = mid + precision\n",
    "        else:\n",
    "            right = mid\n",
    "    \n",
    "    return max_support\n",
    "\n",
    "data = open('kosarak.dat').read()\n",
    "transactions = load_transactions(data)\n",
    "\n",
    "print(\"Part a: Frequent itemsets with minSupport = 0.15\")\n",
    "frequent_sets = apriori(transactions, 0.15)\n",
    "for k, itemsets in frequent_sets.items():\n",
    "    print(f\"\\nFrequent {k}-itemsets:\")\n",
    "    for itemset, support in itemsets.items():\n",
    "        print(f\"Items: {sorted(itemset)}, Support: {support:.3f}\")\n",
    "\n",
    "max_support = find_max_support_size4(transactions)\n",
    "print(f\"\\nPart b: Maximum support value for size-4 itemset: {max_support:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph 1 Results:\n",
      "Hub scores: ['0.5774', '0.5774', '0.5774', '0.0000']\n",
      "Authority scores: ['0.0000', '0.5774', '0.5774', '0.5774']\n",
      "Convergence achieved in 2 steps\n",
      "\n",
      "Graph 2 Results:\n",
      "Hub scores: ['1.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "Authority scores: ['0.0000', '0.5000', '0.5000', '0.5000', '0.5000']\n",
      "Convergence achieved in 2 steps\n",
      "\n",
      "Graph 3 Results:\n",
      "Hub scores: ['0.3945', '0.3945', '0.3945', '0.3945', '0.6143']\n",
      "Authority scores: ['0.3934', '0.3934', '0.3934', '0.3934', '0.6173']\n",
      "Convergence achieved in 5 steps\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 4\n",
    "def normalize(vector):\n",
    "    \"\"\"\n",
    "    Normalize a vector to have a unit norm.\n",
    "    \"\"\"\n",
    "    norm = sum(x * x for x in vector) ** 0.5\n",
    "    return [x / norm if norm > 0 else 0 for x in vector]\n",
    "\n",
    "def hits(adjacency_matrix, tolerance=0.0001, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Compute the HITS algorithm on a given adjacency matrix.\n",
    "    \"\"\"\n",
    "    n = len(adjacency_matrix)\n",
    "    hubs = [1.0] * n\n",
    "    authorities = [1.0] * n\n",
    "    \n",
    "    steps = 0\n",
    "    for _ in range(max_iterations):\n",
    "        steps += 1\n",
    "        \n",
    "        # Store previous values for convergence check\n",
    "        prev_hubs = hubs.copy()\n",
    "        prev_authorities = authorities.copy()\n",
    "        \n",
    "        # Update authorities\n",
    "        new_authorities = [0.0] * n\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if adjacency_matrix[j][i] == 1:\n",
    "                    new_authorities[i] += hubs[j]\n",
    "        \n",
    "        # Update hubs\n",
    "        new_hubs = [0.0] * n\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if adjacency_matrix[i][j] == 1:\n",
    "                    new_hubs[i] += new_authorities[j]\n",
    "        \n",
    "        hubs = normalize(new_hubs)\n",
    "        authorities = normalize(new_authorities)\n",
    "        \n",
    "        # Check for convergence\n",
    "        hub_diff = sum((h1 - h2)**2 for h1, h2 in zip(hubs, prev_hubs))\n",
    "        auth_diff = sum((a1 - a2)**2 for a1, a2 in zip(authorities, prev_authorities))\n",
    "        \n",
    "        if hub_diff < tolerance and auth_diff < tolerance:\n",
    "            break\n",
    "    \n",
    "    return hubs, authorities, steps\n",
    "\n",
    "# Define three different graphs\n",
    "# Graph 1: Simple chain (4 nodes)\n",
    "graph1 = [\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Graph 2: Star topology (5 nodes)\n",
    "graph2 = [\n",
    "    [0, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Graph 3: Cycle with central node (5 nodes)\n",
    "graph3 = [\n",
    "    [0, 1, 0, 0, 1],\n",
    "    [0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 0]\n",
    "]\n",
    "\n",
    "tolerance = 0.0001\n",
    "\n",
    "for i, graph in enumerate([graph1, graph2, graph3], 1):\n",
    "    hubs, authorities, steps = hits(graph, tolerance)\n",
    "    \n",
    "    print(f\"\\nGraph {i} Results:\")\n",
    "    print(\"Hub scores:\", [f\"{x:.4f}\" for x in hubs])\n",
    "    print(\"Authority scores:\", [f\"{x:.4f}\" for x in authorities])\n",
    "    print(f\"Convergence achieved in {steps} steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
