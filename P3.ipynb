{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '3' # for windows threading issue\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are provided with a dataset minerals.csv containing data about the height, width, density,\n",
    "# hardness, and color_intensity of various mineral stones. The first column indicates the class of\n",
    "# each stone (labeled as X, Y, Z, W, or V)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/minerals.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Handling:\n",
    "# Ensure that the dataset is preprocessed (handle any missing values, normalize the features\n",
    "# if required).\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "df_scaled.insert(0, 'Class', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 15 11  8  6]\n",
      " [15 12 10  7 13]\n",
      " [14 15 13 15 11]\n",
      " [14 12 13 13 13]\n",
      " [ 9 18 11  8 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           V       0.21      0.26      0.23        54\n",
      "           W       0.17      0.21      0.19        57\n",
      "           X       0.22      0.19      0.21        68\n",
      "           Y       0.25      0.20      0.22        65\n",
      "           Z       0.19      0.18      0.18        56\n",
      "\n",
      "    accuracy                           0.21       300\n",
      "   macro avg       0.21      0.21      0.21       300\n",
      "weighted avg       0.21      0.21      0.21       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Decision Tree Classification:\n",
    "# a. Randomly split the dataset into 70% training and 30% test sets.\n",
    "# b. Build a classification model using Decision Trees.\n",
    "# c. Generate the confusion matrix and classification report for the test set.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled.drop('Class', axis=1), df_scaled['Class'], test_size=0.3, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18 12 13  8  3]\n",
      " [22 14 10  6  5]\n",
      " [18 18 11 13  8]\n",
      " [23 15 15  7  5]\n",
      " [15 17 12  8  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           V       0.19      0.33      0.24        54\n",
      "           W       0.18      0.25      0.21        57\n",
      "           X       0.18      0.16      0.17        68\n",
      "           Y       0.17      0.11      0.13        65\n",
      "           Z       0.16      0.07      0.10        56\n",
      "\n",
      "    accuracy                           0.18       300\n",
      "   macro avg       0.18      0.18      0.17       300\n",
      "weighted avg       0.18      0.18      0.17       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. K-Nearest Neighbors Classification:\n",
    "# a. Randomly split the dataset into 70% training and 30% test sets.\n",
    "# b. Build a classification model using KNN with K=7.\n",
    "# c. Generate the confusion matrix and classification report for the test set.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled.drop('Class', axis=1), df_scaled['Class'], test_size=0.3, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 15 11  6  9]\n",
      " [10 20 13  3 11]\n",
      " [12 19 12 11 14]\n",
      " [15 12 21  8  9]\n",
      " [ 7 18 12  8 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           V       0.23      0.24      0.23        54\n",
      "           W       0.24      0.35      0.28        57\n",
      "           X       0.17      0.18      0.18        68\n",
      "           Y       0.22      0.12      0.16        65\n",
      "           Z       0.20      0.20      0.20        56\n",
      "\n",
      "    accuracy                           0.21       300\n",
      "   macro avg       0.21      0.22      0.21       300\n",
      "weighted avg       0.21      0.21      0.21       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Support Vector Machine (SVM) Classification:\n",
    "# a. Randomly split the dataset into 70% training and 30% test sets.\n",
    "# b. Build a classification model using SVM with a radial basis function (RBF) kernel.\n",
    "# c. Generate the confusion matrix and classification report for the test set.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled.drop('Class', axis=1), df_scaled['Class'], test_size=0.3, random_state=42)\n",
    "\n",
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 14  9  7 10]\n",
      " [ 9 16 12  5 15]\n",
      " [15 16 11 13 13]\n",
      " [12 21 18  4 10]\n",
      " [14 18  9  3 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           V       0.22      0.26      0.24        54\n",
      "           W       0.19      0.28      0.23        57\n",
      "           X       0.19      0.16      0.17        68\n",
      "           Y       0.12      0.06      0.08        65\n",
      "           Z       0.20      0.21      0.21        56\n",
      "\n",
      "    accuracy                           0.19       300\n",
      "   macro avg       0.18      0.20      0.19       300\n",
      "weighted avg       0.18      0.19      0.18       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Naïve Bayes Classification:\n",
    "# a. Randomly split the dataset into 70% training and 30% test sets.\n",
    "# b. Build a classification model using Gaussian Naïve Bayes method.\n",
    "# c. Generate the confusion matrix and classification report for the test set.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled.drop('Class', axis=1), df_scaled['Class'], test_size=0.3, random_state=42)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Prediction: ['Z']\n",
      "KNN Prediction: ['V']\n",
      "SVM Prediction: ['W']\n",
      "Naive Bayes Prediction: ['W']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but GaussianNB was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 6. Classification of New Data:\n",
    "# Use all four models trained in the tasks above to classify the following new entry: height: 5.8,\n",
    "# width: 3.6, density: 6.9, hardness: 7.4, color_intensity: 8.2. Compare the predicted class for this\n",
    "# entry from all three models and briefly discuss any differences.\n",
    "\n",
    "new_entry = pd.DataFrame({'Height': [5.8], 'Width': [3.6], 'Density': [6.9], 'Hardness': [7.4], 'Color_Intensity': [8.2]})\n",
    "new_entry_scaled = scaler.transform(new_entry)\n",
    "\n",
    "dt_pred = dt.predict(new_entry_scaled)\n",
    "\n",
    "knn_pred = knn.predict(new_entry_scaled)\n",
    "\n",
    "svm_pred = svm.predict(new_entry_scaled)\n",
    "\n",
    "nb_pred = nb.predict(new_entry_scaled)\n",
    "\n",
    "print(f'Decision Tree Prediction: {dt_pred}')\n",
    "print(f'KNN Prediction: {knn_pred}')\n",
    "print(f'SVM Prediction: {svm_pred}')\n",
    "print(f'Naive Bayes Prediction: {nb_pred}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
